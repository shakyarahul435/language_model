{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939d0ea5",
   "metadata": {},
   "source": [
    "# Lstm Language Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "760e0224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# import torchtext\n",
    "from tqdm import tqdm #progress bar \n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "                                                                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ad2bfee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5021e531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1dcb9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 312\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9894a8",
   "metadata": {},
   "source": [
    "## 1. Load Data - Wiki Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b58f938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Sakonii/nepalitext-language-model-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "884b62cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 13141222\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 268189\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9b37e196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 12878397\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 262825\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 268189\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "split = dataset['train'].train_test_split(test_size=0.02, seed=42)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": split['train'],\n",
    "    \"validation\": split['test'],\n",
    "    \"test\": dataset['test']\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1ae0b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].select(range(10_000))\n",
    "dataset[\"validation\"] = dataset[\"validation\"].select(range(1_000))\n",
    "dataset[\"test\"] = dataset[\"test\"].select(range(1_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3fc60111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ल्होसारको अबसरमा सरकारले आज सार्वजनिक विदा दिएको छ।\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][1122]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f44b0e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1)\n",
      "(1000, 1)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'].shape)\n",
    "print(dataset['validation'].shape)\n",
    "print(dataset['test'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98835bd2",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543d5ac9",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Simply tokenize the given text to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a73046b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizer for english text\n",
    "\n",
    "# tokenizer = torchtext.data.utils.get_tokenizer('basic_english') \n",
    "\n",
    "# tokenize_data = lambda example , tokenizer: {'tokens': tokenizer(example['text'])}\n",
    "\n",
    "# tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': tokenizer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0472d339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:00<00:00, 29276.67 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 22213.95 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 33865.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "## tokenizer for nepali text\n",
    "\n",
    "def nepali_tokenizer(text):\n",
    "    return text.strip().split()\n",
    "\n",
    "def tokenize_data(example):\n",
    "    return {\"tokens\": nepali_tokenizer(example[\"text\"])}\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_data,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b065075a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ल्होसारको', 'अबसरमा', 'सरकारले', 'आज', 'सार्वजनिक', 'विदा', 'दिएको', 'छ।']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset['train'][1122]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac7a6dd",
   "metadata": {},
   "source": [
    "## Numericalizing\n",
    "We will tell torchtext to add any word that has occurred at least three times in the dataset to the vocabulary because otherwise it would be too big. Also we shall make sure to add <b style=color:yellow>unk</b> and <b style=color:yellow>eos</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ae76bc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## torchtext depreciated not working at all \n",
    "# vocab = torchtext.vocab.build_vocab_for_iterator(tokenized_dataset['train']['tokens'], min_freq=3)\n",
    "\n",
    "# vocab.insert_token('<unk>', 0)\n",
    "# vocab.insert_token('<eos>', 1)\n",
    "# vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "62c37ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "for tokens in tokenized_dataset[\"train\"][\"tokens\"]:\n",
    "    counter.update(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "56beff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    \"<unk>\": 0,\n",
    "    \"<eos>\": 1,\n",
    "}\n",
    "\n",
    "for token, freq in counter.items():\n",
    "    if freq >= 3 and token not in vocab:\n",
    "        vocab[token] = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fdc701b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "itos = list(vocab.keys())\n",
    "with open('itos.pkl', 'wb') as f:\n",
    "    pickle.dump(itos, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1716351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(tokens):\n",
    "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "18e6e44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2403, 6574, 0, 1]\n",
      "12905\n"
     ]
    }
   ],
   "source": [
    "ids = numericalize([\"Nepal\", \"is\", \"beautiful\", \"<eos>\"])\n",
    "print(ids)\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f9f1a353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<eos>',\n",
       " 'पितृ',\n",
       " 'हाम्रा',\n",
       " 'देवता',\n",
       " 'हुन्',\n",
       " '।',\n",
       " 'सम्मान',\n",
       " 'गर्न',\n",
       " 'सकियो',\n",
       " 'भने',\n",
       " 'मात्र',\n",
       " 'हाम्रो',\n",
       " 'कल्याण',\n",
       " 'सम्भव',\n",
       " 'छ',\n",
       " 'आजको',\n",
       " 'सुनौलो',\n",
       " 'वर्तमान',\n",
       " 'दिने',\n",
       " 'र',\n",
       " 'सिकाउने',\n",
       " 'इतिहास',\n",
       " 'भए',\n",
       " 'पनि',\n",
       " 'हुन्छ',\n",
       " 'नै',\n",
       " 'भोलिको',\n",
       " 'सुन्दर',\n",
       " 'भविष्य',\n",
       " 'हो',\n",
       " 'मार्गदर्शक',\n",
       " 'तिनै',\n",
       " 'आज',\n",
       " 'ज्ञान',\n",
       " 'विज्ञानको',\n",
       " 'विकास',\n",
       " 'भइरहेको',\n",
       " 'अतः',\n",
       " 'मानव',\n",
       " 'निम्ति',\n",
       " 'श्रद्धा',\n",
       " 'भाव',\n",
       " 'व्यक्त',\n",
       " 'गर्नु',\n",
       " 'आवश्यक',\n",
       " '४',\n",
       " 'मंसिर',\n",
       " '२०७५,',\n",
       " 'मंगलवार',\n",
       " 'अध्यक्ष',\n",
       " 'मण्डलको',\n",
       " 'नेतृत्व',\n",
       " 'प्रणालीमा',\n",
       " 'सञ्चालन',\n",
       " 'गर्ने',\n",
       " 'राजपाको',\n",
       " 'निर्णय',\n",
       " 'काठमाडौं',\n",
       " ':',\n",
       " 'सरकारले',\n",
       " 'एकै',\n",
       " 'दिन',\n",
       " '५०',\n",
       " 'करोड',\n",
       " 'अमेरिकी',\n",
       " 'डलर',\n",
       " 'बराबरको',\n",
       " 'ऋण',\n",
       " 'तथा',\n",
       " 'अनुदान',\n",
       " 'सहायता',\n",
       " 'स्वीकृत',\n",
       " 'गरेको',\n",
       " 'छ।',\n",
       " 'विभिन्न',\n",
       " 'दातृ',\n",
       " 'विश्वकप',\n",
       " 'फुटबलमा',\n",
       " 'ब्राजिल',\n",
       " 'बेल्जियम',\n",
       " 'क्वाटर',\n",
       " 'दुई',\n",
       " 'खेल',\n",
       " 'हुँदै',\n",
       " 'सामान्य',\n",
       " 'अर्थमा',\n",
       " 'कोही',\n",
       " 'गरिने',\n",
       " 'यौन',\n",
       " 'व्यवहार',\n",
       " 'व्यक्तिको',\n",
       " 'बाहिर',\n",
       " 'जानु',\n",
       " 'सेक्स',\n",
       " 'मानिन्छ',\n",
       " 'यस्तो',\n",
       " 'बसेका',\n",
       " 'मानिस',\n",
       " 'अधिकांश',\n",
       " 'समय',\n",
       " 'बारेमा',\n",
       " 'सोचेर',\n",
       " 'अश्लील',\n",
       " 'फिल्म',\n",
       " 'वा',\n",
       " 'साइट',\n",
       " 'हेर्नु',\n",
       " 'उनीहरूको',\n",
       " 'बन्न',\n",
       " 'सक्छ',\n",
       " 'यसले',\n",
       " 'मानिसको',\n",
       " 'व्यवहारमा',\n",
       " 'समेत',\n",
       " 'परिवर्तन',\n",
       " 'आउन',\n",
       " 'थाल्छ',\n",
       " 'यसको',\n",
       " 'पारिवारिक',\n",
       " 'सामाजिक',\n",
       " 'जीवन',\n",
       " 'अस्तव्यस्त',\n",
       " 'रूप',\n",
       " 'लियो',\n",
       " '...',\n",
       " 'सञ्चारकर्मी',\n",
       " 'प्रेमको',\n",
       " 'गीत',\n",
       " '५',\n",
       " 'मा',\n",
       " 'पर्न',\n",
       " 'सफल',\n",
       " 'यो',\n",
       " 'हैन,',\n",
       " 'राजनैतिक',\n",
       " 'आस्थाका',\n",
       " 'आधारमा',\n",
       " 'मानिसहरू',\n",
       " 'प्रकट',\n",
       " 'हुनेछन्',\n",
       " 'भन्ने',\n",
       " 'मैले',\n",
       " 'कहिल्यै',\n",
       " 'सोचेको',\n",
       " 'सायद',\n",
       " 'म',\n",
       " 'गलत',\n",
       " 'थिएँ',\n",
       " 'लागेको',\n",
       " 'थियो,',\n",
       " 'सूचना',\n",
       " 'निकै',\n",
       " 'छोटो',\n",
       " 'समयमै',\n",
       " 'वरिष्ठ',\n",
       " 'संगीतकार',\n",
       " 'विक्रम',\n",
       " 'गुरुङको',\n",
       " 'सांगीतिक',\n",
       " 'कार्यक्रम',\n",
       " '‘मेरो',\n",
       " 'संगीत',\n",
       " '?',\n",
       " 'माघ',\n",
       " '५,',\n",
       " '२०७४',\n",
       " 'खसी',\n",
       " 'अनुसार',\n",
       " 'करिब',\n",
       " 'सय',\n",
       " 'देखी',\n",
       " 'चार',\n",
       " 'सम्म',\n",
       " 'भन्सार',\n",
       " 'पर्ने',\n",
       " 'हुदा',\n",
       " 'भाउ',\n",
       " 'बताउछन्',\n",
       " 'पानी',\n",
       " 'खुवाउने',\n",
       " 'हुन',\n",
       " 'नदिने',\n",
       " 'बताए',\n",
       " 'मोरङ',\n",
       " '-',\n",
       " 'फुटबल',\n",
       " 'एकेडेमी',\n",
       " 'मोरङको',\n",
       " 'बेलबारी',\n",
       " 'नगरपालिका',\n",
       " '११',\n",
       " 'लक्ष्मीमार्गमा',\n",
       " 'जारी',\n",
       " 'दोस्रो',\n",
       " 'अन्तराष्ट्रिय',\n",
       " 'आमन्त्रण',\n",
       " 'गोल्डकपको',\n",
       " 'फाइनलमा',\n",
       " 'प्रवेश',\n",
       " 'रहेकाे',\n",
       " 'मैदानमा',\n",
       " 'बिहीबार',\n",
       " 'भएको',\n",
       " 'पहिलो',\n",
       " 'सेमिफाइनल',\n",
       " 'खेलमा',\n",
       " 'चर्च',\n",
       " 'ब्वाइज',\n",
       " 'युनाइटेड',\n",
       " 'काठमाडौंलाई',\n",
       " '२–१',\n",
       " 'गोल',\n",
       " 'अन्तरले',\n",
       " 'हराउँदै',\n",
       " 'उपाधि',\n",
       " 'होडमा',\n",
       " 'पुगेको',\n",
       " 'उसले',\n",
       " 'आगामी',\n",
       " 'शनिबार',\n",
       " 'रेड',\n",
       " 'हर्स',\n",
       " 'क्लब',\n",
       " 'इलाम',\n",
       " 'विर्तामोड',\n",
       " 'विजेताको',\n",
       " 'पर्नेछ',\n",
       " 'जितका',\n",
       " 'लागि',\n",
       " 'दुवै',\n",
       " 'हाफमा',\n",
       " 'उत्कृष्ट',\n",
       " 'प्रदर्शन',\n",
       " 'गरे',\n",
       " 'पाएको',\n",
       " 'अवसरलाई',\n",
       " 'सदुपयोग',\n",
       " 'नसक्दा',\n",
       " 'नतिजा',\n",
       " 'पक्षमा',\n",
       " 'पुग्यो',\n",
       " 'खेललाई',\n",
       " 'सुधार',\n",
       " 'मोरङले',\n",
       " 'संख्या',\n",
       " 'तीन',\n",
       " 'पुर्\\u200dयाउन',\n",
       " 'सकेन',\n",
       " 'राई',\n",
       " 'विदेशी',\n",
       " 'खेलाडी',\n",
       " 'प्रेकले',\n",
       " 'एक-एक',\n",
       " 'ब्वाइजका',\n",
       " 'तर्फबाट',\n",
       " 'निरज',\n",
       " 'बस्नेतले',\n",
       " 'एक',\n",
       " 'फर्काए',\n",
       " 'पर्याप्त',\n",
       " 'हाफको',\n",
       " 'अन्त्यमा',\n",
       " 'अर्को',\n",
       " 'बन्यो',\n",
       " 'एक्लै',\n",
       " 'बल',\n",
       " 'लिएर',\n",
       " 'अघि',\n",
       " 'बढेका',\n",
       " 'वान',\n",
       " 'भर्सेस',\n",
       " 'स्थितिमा',\n",
       " 'गाेल',\n",
       " 'गरेपछि',\n",
       " 'कुनै',\n",
       " '६९',\n",
       " 'औं',\n",
       " 'मिनेटमा',\n",
       " 'प्रहार',\n",
       " 'खेलको',\n",
       " 'म्यान',\n",
       " 'अफ',\n",
       " 'दी',\n",
       " 'म्याच',\n",
       " 'मोरङका',\n",
       " 'विवेक',\n",
       " 'पौडेललाई',\n",
       " 'उर्लाबारी',\n",
       " 'नगरपालिकाका',\n",
       " 'मेयर',\n",
       " 'खड्ग',\n",
       " 'पुरस्कार',\n",
       " 'प्रदान',\n",
       " 'शुक्रबारको',\n",
       " 'प्रतिष्पर्धा',\n",
       " 'हुने',\n",
       " 'सुमन',\n",
       " 'तिमल्सिना',\n",
       " 'विराटनगरको',\n",
       " 'काम',\n",
       " 'गर्नुहुन्छ',\n",
       " 'उहाँलाई',\n",
       " 'फेसबुकमा',\n",
       " 'भेट्न',\n",
       " 'सकिन्छ',\n",
       " 'जान',\n",
       " 'नसक्ने',\n",
       " 'सहयोगको',\n",
       " 'हेर्नुहोस्)',\n",
       " 'अर्थ',\n",
       " 'सबैका',\n",
       " 'भर्चुअल',\n",
       " 'रुपमा',\n",
       " 'संसार',\n",
       " 'बुझ्न',\n",
       " 'स्थानमा',\n",
       " 'घुम्न',\n",
       " 'चाहनेहरुका',\n",
       " 'उपयोगी',\n",
       " 'शैक्षिक',\n",
       " 'उपकरण',\n",
       " 'स्टोरी',\n",
       " 'साइकल',\n",
       " 'सहकार्यमा',\n",
       " 'कार्यले',\n",
       " 'मलाई',\n",
       " 'धेरै',\n",
       " 'उर्जा',\n",
       " 'दिएको',\n",
       " 'छ”,',\n",
       " 'ढकालले',\n",
       " 'भन्नुभयो',\n",
       " 'राष्ट्रिय',\n",
       " 'सेवा',\n",
       " 'दल',\n",
       " 'विमानस्थल',\n",
       " '२१',\n",
       " 'घण्टा',\n",
       " 'संचालनमा',\n",
       " 'ल्याउने',\n",
       " 'एप्पलले',\n",
       " 'आइफोन',\n",
       " '८',\n",
       " '/',\n",
       " 'लाई',\n",
       " 'गर्दै',\n",
       " 'हे',\n",
       " '!',\n",
       " 'प्रबुद्ध',\n",
       " 'जुन',\n",
       " 'निर्धारित',\n",
       " 'गरेका',\n",
       " 'छन्,',\n",
       " 'त्यसको',\n",
       " 'पालन',\n",
       " 'गर',\n",
       " '_',\n",
       " 'उनै',\n",
       " 'उनैको',\n",
       " 'ग्रन्थ',\n",
       " 'यदी',\n",
       " 'तिमीहरूले',\n",
       " 'यस',\n",
       " 'सकेको',\n",
       " 'बारे',\n",
       " 'यस्तै',\n",
       " 'हत्यामा',\n",
       " 'संलग्न',\n",
       " 'आरोपमा',\n",
       " 'भारत,',\n",
       " 'पाण्डे,',\n",
       " 'पर्साका',\n",
       " 'महतो',\n",
       " 'सन्तोष',\n",
       " 'गुरुङ',\n",
       " 'काठमाडौंका',\n",
       " 'प्रकाश',\n",
       " 'बुढाथोकी',\n",
       " 'पक्राउ',\n",
       " 'परेका',\n",
       " 'छन्',\n",
       " 'प्रहरीका',\n",
       " 'पाण्डे',\n",
       " 'महतोले',\n",
       " 'हतियार',\n",
       " 'थिए',\n",
       " 'गुरुङले',\n",
       " 'प्रयोग',\n",
       " 'मोटरसाइकल',\n",
       " 'जोहो',\n",
       " 'सहयोगी',\n",
       " 'स्वतन्त्रता',\n",
       " 'ओली',\n",
       " 'सरकार',\n",
       " 'युवा',\n",
       " 'संघ,',\n",
       " 'नेपाल',\n",
       " 'पढिएको',\n",
       " 'हो।',\n",
       " 'को',\n",
       " 'मध्य',\n",
       " 'भाग',\n",
       " 'तर',\n",
       " 'यसैमा',\n",
       " 'त',\n",
       " 'रमाइलो',\n",
       " '–',\n",
       " 'रेगुलर',\n",
       " 'भैहाल्यो',\n",
       " 'नि।',\n",
       " 'फेरी',\n",
       " 'सब',\n",
       " 'कुराको',\n",
       " 'एउटा',\n",
       " 'आफ्नै',\n",
       " 'सीमा',\n",
       " 'शुरु',\n",
       " 'भयो',\n",
       " 'याक',\n",
       " '१०',\n",
       " 'देशका',\n",
       " '३१',\n",
       " 'जना',\n",
       " 'सहभागी',\n",
       " 'अबको',\n",
       " 'मेरो',\n",
       " 'अध्ययन',\n",
       " 'लेखन',\n",
       " 'प्रदीप',\n",
       " 'प्रतिक्रिया',\n",
       " 'दिनुहोस्',\n",
       " 'संघको',\n",
       " 'निर्वाचनमा',\n",
       " 'समानुपातिकतर्फ',\n",
       " 'काँग्रेसले',\n",
       " 'तुलसी',\n",
       " 'सांसद',\n",
       " 'गणेश',\n",
       " 'सभापति',\n",
       " 'रामनारायण',\n",
       " 'ढुंगाना,',\n",
       " 'भट्टराई',\n",
       " 'नाम',\n",
       " 'सिफारिस',\n",
       " 'प्रादेशिक',\n",
       " 'क्षेत्र',\n",
       " 'नं',\n",
       " '१',\n",
       " 'प्रत्यक्षतर्फ',\n",
       " 'जिल्ला',\n",
       " 'गोविन्द',\n",
       " 'गरिएको',\n",
       " 'एभरेस्ट',\n",
       " 'प्रिमियर',\n",
       " 'लिग',\n",
       " '(',\n",
       " 'इपिएल',\n",
       " ')',\n",
       " 'तेस्रो',\n",
       " 'संस्करण',\n",
       " '२२',\n",
       " 'मङ्सिर',\n",
       " 'पुष',\n",
       " '७',\n",
       " 'आयोजना',\n",
       " 'राजधानीमा',\n",
       " 'आयोजित',\n",
       " 'पत्रकार',\n",
       " 'प्रतियोगिताको',\n",
       " 'मिति',\n",
       " 'घोषणा',\n",
       " 'कार्यक्रममा',\n",
       " 'निर्देशक',\n",
       " 'पटक',\n",
       " 'भब्य',\n",
       " 'बनाउन',\n",
       " 'निम्न',\n",
       " 'कुराहरु',\n",
       " 'गरिरहेको',\n",
       " 'बताएका',\n",
       " 'प्रतियोगिता',\n",
       " '१३',\n",
       " 'बाट',\n",
       " 'बढाएर',\n",
       " '१५',\n",
       " 'बनाउदा',\n",
       " 'उद्घाटन',\n",
       " 'देखि',\n",
       " '३',\n",
       " 'वटा',\n",
       " 'जसले',\n",
       " 'गर्दा',\n",
       " 'अझै',\n",
       " 'दर्शकहरु',\n",
       " 'उपस्थित',\n",
       " 'अपेक्षा',\n",
       " 'अझ',\n",
       " 'चरणको',\n",
       " 'खेलहरु',\n",
       " 'दिनमा',\n",
       " 'मात्रै',\n",
       " 'प्रयास',\n",
       " 'गरिनेछ।',\n",
       " 'क्रिकेट',\n",
       " 'विकास,',\n",
       " 'लगायत',\n",
       " 'गरेर',\n",
       " '४६',\n",
       " 'पृष्ठ',\n",
       " 'राखिएको',\n",
       " 'अघिल्लो',\n",
       " 'बर्षको',\n",
       " 'आयोजनाको',\n",
       " 'बिभिन्न',\n",
       " 'कोणबाट',\n",
       " 'विश्लेषण',\n",
       " 'रिपोर्ट',\n",
       " 'पठाएका',\n",
       " 'जसलाई',\n",
       " 'हेरेर',\n",
       " 'प्रभावित',\n",
       " 'जवाफ',\n",
       " 'पठाएको',\n",
       " '२०',\n",
       " 'हजार',\n",
       " 'छौ',\n",
       " 'अनुमति',\n",
       " 'बिश्वास',\n",
       " 'बिदेशी',\n",
       " 'राम्रो',\n",
       " 'स्तरको',\n",
       " 'ल्याउन',\n",
       " 'जोड',\n",
       " 'महिना',\n",
       " 'देखिनै',\n",
       " 'गरिसकेका',\n",
       " 'छन्।',\n",
       " 'हैन',\n",
       " 'प्रशिक्षक',\n",
       " 'आउदा',\n",
       " 'त्यसले',\n",
       " 'ग्लोबल',\n",
       " 'नेपाली',\n",
       " 'प्रस्तुत',\n",
       " 'गर्नेछ।',\n",
       " 'कोहि',\n",
       " 'पैसाको',\n",
       " 'नभई',\n",
       " 'नेपालमा',\n",
       " 'केहि',\n",
       " 'दर्शकहरुको',\n",
       " 'कारण',\n",
       " 'खेलाडीहरुको',\n",
       " 'बाहेक',\n",
       " 'घुम्ने',\n",
       " 'प्रतियोगितामा',\n",
       " 'अंग्रेजी',\n",
       " 'चलेका',\n",
       " '२०१६',\n",
       " '१७',\n",
       " 'सिजनमा',\n",
       " '२०१८',\n",
       " 'राशिको',\n",
       " 'हामि',\n",
       " 'पहिला',\n",
       " 'त्यो',\n",
       " 'आकर्षक',\n",
       " 'कुरा',\n",
       " 'थियो',\n",
       " 'होला',\n",
       " 'अब',\n",
       " 'क्रिकेटको',\n",
       " 'स्तर',\n",
       " 'ध्यान',\n",
       " 'राशि',\n",
       " 'बढ्नेछ',\n",
       " 'हिसाबले',\n",
       " 'हेर्ने',\n",
       " 'अगाडी',\n",
       " 'आएर',\n",
       " 'बस्ने',\n",
       " 'भन्दा',\n",
       " 'महत्वपूर्ण',\n",
       " 'हुन्छ।',\n",
       " 'लिगमा',\n",
       " 'कुल',\n",
       " '६',\n",
       " 'जसमा',\n",
       " 'चितवन',\n",
       " 'भैरहवा',\n",
       " 'पोखरा',\n",
       " 'काठमाडौँ',\n",
       " 'ललितपुर',\n",
       " 'रहेका',\n",
       " 'पछिल्लो',\n",
       " 'जस्तै',\n",
       " 'प्रत्येक',\n",
       " 'टिमले',\n",
       " 'गत',\n",
       " 'बर्ष',\n",
       " 'योगेश',\n",
       " 'पुनित',\n",
       " 'जस्ता',\n",
       " 'खेलाडीहरुले',\n",
       " 'खेलेका',\n",
       " 'थिए।',\n",
       " 'सिजनको',\n",
       " 'पारस',\n",
       " 'खड्का',\n",
       " 'शरद',\n",
       " 'रनले',\n",
       " 'पराजित',\n",
       " 'जितेको',\n",
       " 'थियो।',\n",
       " 'निर्वाचन',\n",
       " 'आयोगले',\n",
       " 'माओवादीले',\n",
       " 'टुंगो',\n",
       " 'लगाएर',\n",
       " 'आयोगमा',\n",
       " 'बुझाएको',\n",
       " 'नाश',\n",
       " 'भएका',\n",
       " 'बास',\n",
       " 'हामीले',\n",
       " 'सुनेका',\n",
       " 'छौं',\n",
       " 'कति',\n",
       " 'दुख',\n",
       " 'लाग्ने',\n",
       " 'परिवारलाई',\n",
       " 'मार्न',\n",
       " 'कस्तो',\n",
       " 'ठूलो',\n",
       " 'अपराध',\n",
       " 'थालेका',\n",
       " 'रहेछौं',\n",
       " 'काठमाडौ,',\n",
       " 'असोज',\n",
       " 'नागरिक',\n",
       " 'उड्डयन',\n",
       " 'प्राधिकरणका',\n",
       " 'महानिर्देशक',\n",
       " 'सञ्जीव',\n",
       " 'गौतमले',\n",
       " 'बुझाए',\n",
       " 'लगत्तै',\n",
       " 'राजनीतिक',\n",
       " 'वृत्तमा',\n",
       " 'नयाँ',\n",
       " 'तरंग',\n",
       " 'पैदा',\n",
       " 'खानेपानी',\n",
       " 'व्यवस्था',\n",
       " 'गर्नेलाई',\n",
       " 'मत',\n",
       " 'हाल्ने',\n",
       " 'बताइन्',\n",
       " 'उनले',\n",
       " 'भनिन्',\n",
       " '।’',\n",
       " 'गाउँ',\n",
       " 'नजिक',\n",
       " 'पर्छन्',\n",
       " 'होटल',\n",
       " 'आएकी',\n",
       " 'मगर',\n",
       " 'नेताहरूको',\n",
       " 'प्रवृत्ति',\n",
       " 'देख्दा',\n",
       " 'मतदान',\n",
       " 'मन',\n",
       " 'नलाग्ने',\n",
       " 'बताउँछिन्',\n",
       " 'जस्तो',\n",
       " 'गर्छन्,',\n",
       " 'नेताले',\n",
       " 'चित्तबुझ्दो',\n",
       " 'गरेकै',\n",
       " 'भनिन्,',\n",
       " '‘तर,',\n",
       " 'गाउँमै',\n",
       " 'आउने',\n",
       " 'भएकाले',\n",
       " 'भोट',\n",
       " 'हाल्नुपर्छ',\n",
       " 'लाग्छ',\n",
       " 'सडकको',\n",
       " 'ट्रयाक',\n",
       " 'खोल्न',\n",
       " 'रकम',\n",
       " 'साबिक',\n",
       " 'खर्च',\n",
       " 'भएर',\n",
       " 'सदरमुकाम',\n",
       " 'जाने',\n",
       " 'सडकका',\n",
       " 'स्तरोन्नति',\n",
       " 'नभएको',\n",
       " 'गुनासो',\n",
       " 'स्थानीयको',\n",
       " 'यहाँ',\n",
       " 'खानेपानी,',\n",
       " 'विद्यालय',\n",
       " 'सुविधा',\n",
       " 'छैन',\n",
       " 'क्षेत्रमा',\n",
       " 'खानेपानीको',\n",
       " 'लक्ष्मी',\n",
       " 'चिनियाँ',\n",
       " 'अन्तर्राष्ट्रिय',\n",
       " 'रेडियोको',\n",
       " 'सेवा,',\n",
       " 'रूपमा',\n",
       " 'कार्यरत',\n",
       " 'सुरक्षा',\n",
       " 'अचम्मको',\n",
       " 'उपाय',\n",
       " 'किन',\n",
       " 'भ्रममा',\n",
       " 'आफू',\n",
       " 'परेको',\n",
       " 'कारणहरु',\n",
       " 'उनीहरुले',\n",
       " 'त्यस्तो',\n",
       " 'प्रमाण',\n",
       " 'संकलन',\n",
       " 'चाहेका',\n",
       " 'थिए,',\n",
       " 'सही',\n",
       " 'दशकअघि',\n",
       " 'पसेका',\n",
       " 'दीपक',\n",
       " 'अहिले',\n",
       " 'पोखराका',\n",
       " 'उद्यमी',\n",
       " 'मध्येका',\n",
       " 'तले',\n",
       " 'घर',\n",
       " 'बनाएका',\n",
       " 'एग्रो',\n",
       " 'करीब',\n",
       " '२',\n",
       " 'भैंसी',\n",
       " 'जानकी',\n",
       " 'कटाएर',\n",
       " 'मासिक',\n",
       " 'लाख',\n",
       " 'कमाउँछन्',\n",
       " 'यद्यपि',\n",
       " 'लगानीको',\n",
       " 'ब्याज',\n",
       " 'घटाउने',\n",
       " 'आम्दानीको',\n",
       " 'आधा',\n",
       " 'खुद',\n",
       " 'मुनाफा',\n",
       " 'उनी',\n",
       " 'बताउँछन्',\n",
       " 'राजविराज,',\n",
       " 'कात्तिक',\n",
       " 'मैथिली',\n",
       " 'साहित्य',\n",
       " 'परिषद',\n",
       " 'स्मृति',\n",
       " 'पर्व',\n",
       " 'का',\n",
       " 'सम्पूर्ण',\n",
       " 'स्थगन',\n",
       " 'विशेष',\n",
       " 'आफ्ना',\n",
       " 'कार्यक्रमहरु',\n",
       " 'हस्तान्तरण',\n",
       " 'आफ्नो',\n",
       " 'चाहना',\n",
       " 'पार्टी',\n",
       " 'पूरा',\n",
       " 'नभएकाले',\n",
       " 'विधि',\n",
       " 'प्रक्रिया',\n",
       " 'आफूले',\n",
       " 'चाहेको',\n",
       " 'लामाले',\n",
       " 'व्यवस्थापन',\n",
       " 'पार्टीले',\n",
       " 'बिदा',\n",
       " 'लिन',\n",
       " 'खोजेको',\n",
       " 'सडकमा',\n",
       " 'कसैलाई',\n",
       " 'बिधि',\n",
       " 'प्रक्रियाबाट',\n",
       " 'अभिनय',\n",
       " 'अरु',\n",
       " 'के',\n",
       " 'निकालिएको',\n",
       " 'कोलेस्टेरोल',\n",
       " 'मद्दत',\n",
       " 'गर्छ',\n",
       " 'अमेरिकन',\n",
       " 'अनुसन्धानमा',\n",
       " 'नियमित',\n",
       " 'कालो',\n",
       " 'चिया',\n",
       " 'सेवन',\n",
       " 'गर्नेमा',\n",
       " 'खराब',\n",
       " 'घटेको',\n",
       " 'पाइएको',\n",
       " 'मुख्य',\n",
       " 'मानिन्छ।',\n",
       " 'विकासका',\n",
       " 'उत्तर',\n",
       " 'कोरिया',\n",
       " 'दक्षिण',\n",
       " 'बीचको',\n",
       " '६५',\n",
       " 'वर्षे',\n",
       " 'अन्त्य',\n",
       " 'सहमति',\n",
       " 'प्रधानमन्त्रीले',\n",
       " 'ढोका',\n",
       " 'बहुपक्षीय',\n",
       " 'प्राविधिक',\n",
       " 'आर्थिक',\n",
       " 'सहयोगका',\n",
       " 'चौथो',\n",
       " 'शिखर',\n",
       " 'सम्मेलन',\n",
       " 'शुरू',\n",
       " 'समाचार',\n",
       " 'चुनावी',\n",
       " 'प्रचारमा',\n",
       " 'आए',\n",
       " 'दिपक',\n",
       " 'जिल्लामा',\n",
       " 'वातावरण',\n",
       " 'एमाले',\n",
       " 'नेता',\n",
       " 'समुहले',\n",
       " 'स्थानीयलाई',\n",
       " 'घटना',\n",
       " 'सार्वजनिक',\n",
       " 'ठाउँका',\n",
       " 'हात',\n",
       " 'सम्मको',\n",
       " 'धम्की',\n",
       " 'जानकारी',\n",
       " 'प्राप्त',\n",
       " 'कोरियाले',\n",
       " 'बारम्बार',\n",
       " 'आणविक',\n",
       " 'हतियारको',\n",
       " 'परीक्षण',\n",
       " 'आएकाले',\n",
       " 'जापान',\n",
       " 'अमेरिका',\n",
       " 'बढी',\n",
       " 'पिएनपि',\n",
       " 'खबर',\n",
       " 'डट',\n",
       " 'कम',\n",
       " 'वेवसाइट',\n",
       " 'पि.',\n",
       " 'एन.',\n",
       " 'मिडिया',\n",
       " 'प्रा.',\n",
       " 'लि.',\n",
       " 'आधिकारिक',\n",
       " 'न्यूज',\n",
       " 'पोर्टल',\n",
       " 'भाषाको',\n",
       " 'वेवसाइटले',\n",
       " 'समाचार,',\n",
       " 'मनोरञ्जन,',\n",
       " 'अर्थ,',\n",
       " 'खेलकुद,',\n",
       " 'विश्व,',\n",
       " 'बिज्ञान,',\n",
       " 'प्रविधि,',\n",
       " 'स्वास्थ्य,',\n",
       " 'प्रवास,',\n",
       " 'भिडियो',\n",
       " 'बिबिध',\n",
       " 'समेट्ने',\n",
       " 'गर्दछ।',\n",
       " 'निर्माता',\n",
       " 'शर्त',\n",
       " 'अहिलेसम्म',\n",
       " 'गर्नुहुन्थ्यो',\n",
       " 'पत्रपत्रिका',\n",
       " 'पढ्न',\n",
       " 'विशेषतः',\n",
       " 'किताब',\n",
       " 'पढ्ने',\n",
       " 'गरेकी',\n",
       " 'छु',\n",
       " 'कथा',\n",
       " 'प्राथमिकतामा',\n",
       " 'रुचि',\n",
       " 'दृष्टिगत',\n",
       " 'अमेरिकामा',\n",
       " 'छोराले',\n",
       " 'पठाइदिने',\n",
       " 'त्यस्तै',\n",
       " 'तालिममा',\n",
       " 'सडक',\n",
       " 'डिभिजन',\n",
       " 'कार्यालय',\n",
       " 'रिजालले',\n",
       " 'सहजीकरण',\n",
       " 'गर्नुभएको',\n",
       " 'समस्या',\n",
       " 'समाधान',\n",
       " 'हरेक',\n",
       " 'समयमा',\n",
       " 'प्रमुख',\n",
       " 'खनालले',\n",
       " 'निर्देशन',\n",
       " 'दिनुभएको',\n",
       " 'सो',\n",
       " 'अवसरमा',\n",
       " 'कोलकातामा',\n",
       " 'रहनुभएका',\n",
       " 'नेपालका',\n",
       " 'नृत्य',\n",
       " 'दिलीप',\n",
       " 'थापा',\n",
       " 'निर्देशनमा',\n",
       " 'सांस्कृतिक',\n",
       " 'नाटक',\n",
       " 'असार',\n",
       " '२९',\n",
       " 'गते',\n",
       " 'समितिका',\n",
       " 'महासचिव',\n",
       " 'रामबहादुर',\n",
       " 'कार्कीले',\n",
       " 'भारतमा',\n",
       " 'सम्बन्ध',\n",
       " 'कायम',\n",
       " 'बताउनुभयो',\n",
       " 'नेपालगन्ज,',\n",
       " 'आसन्न',\n",
       " 'स्थानीय',\n",
       " 'तहको',\n",
       " 'निर्वाचनका',\n",
       " 'उम्मेदवारी',\n",
       " 'दर्ता',\n",
       " 'भएसँगै',\n",
       " 'चुनाव',\n",
       " 'भन्नेमा',\n",
       " 'बाँकेका',\n",
       " 'ढुक्क',\n",
       " 'नायिका',\n",
       " 'कोइरालाले',\n",
       " 'नेपालको',\n",
       " 'क्यान्सर',\n",
       " 'रोग',\n",
       " 'डा',\n",
       " 'तारा',\n",
       " 'मानन्धर,',\n",
       " 'रोटरी',\n",
       " 'मल्ल,',\n",
       " 'श्रेष्ठ,',\n",
       " 'आचार्य',\n",
       " 'गुरुङलाई',\n",
       " '(रासस)',\n",
       " 'बलियो',\n",
       " 'प्रविधिमा',\n",
       " 'आधारित',\n",
       " 'निर्माणका',\n",
       " 'लागत',\n",
       " 'बेलैमा',\n",
       " 'उपचार',\n",
       " 'आँखा',\n",
       " 'ठीक',\n",
       " 'तपाईंको',\n",
       " 'अनुसारको',\n",
       " 'विधा',\n",
       " 'रोज्नुहोस्',\n",
       " 'देश',\n",
       " 'विरामी',\n",
       " 'राजा',\n",
       " 'जसको',\n",
       " 'आँखामा',\n",
       " 'समृद्धिको',\n",
       " 'गरी',\n",
       " 'तपाईको',\n",
       " 'सानो',\n",
       " 'ड्रप-शिपिंग',\n",
       " 'लागी',\n",
       " 'व्यवसायको',\n",
       " 'खोजी',\n",
       " 'इञ्जिन',\n",
       " 'अनुकूलन',\n",
       " 'कसरी',\n",
       " 'छ?',\n",
       " 'आज,',\n",
       " ...]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos = list(vocab.keys())\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "90f50cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<eos>', 'पितृ', 'हाम्रा', 'देवता', 'हुन्', '।', 'सम्मान', 'गर्न', 'सकियो']\n"
     ]
    }
   ],
   "source": [
    "print(itos[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee02bd25",
   "metadata": {},
   "source": [
    "## 3. Prepare the batch loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b4d1b",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5a8e3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data(dataset, vocab, batch_size):\n",
    "#     data = []\n",
    "#     # for example in dataset:\n",
    "#     #     if example['tokens']:\n",
    "#     #         tokens = example['tokens'].append('<eos>')\n",
    "#     #         tokens = [vocab[token] for token in example['tokens']]\n",
    "#     #         data.extend(tokens)\n",
    "#     for example in dataset:\n",
    "#         if example['tokens']:\n",
    "#             tokens = example['tokens'] + ['<eos>']   # no append()\n",
    "#             ids = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "#             data.extend(ids)\n",
    "#     data = torch.LongTensor(data)\n",
    "#     num_batches = data.shape[0] // batch_size\n",
    "#     data = data[:num_batches * batch_size]\n",
    "#     data = data.view(batch_size, num_batches)   # view vs. reshape (whether data is contiguous)\n",
    "#     return data # [batch_size, seq_len]\n",
    "\n",
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "    unk = vocab[\"<unk>\"]\n",
    "\n",
    "    for example in dataset:\n",
    "        if example[\"tokens\"]:\n",
    "            tokens = example[\"tokens\"] + [\"<eos>\"]\n",
    "            ids = [vocab.get(token, unk) for token in tokens]\n",
    "            data.extend(ids)\n",
    "\n",
    "    data = torch.LongTensor(data)\n",
    "\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches * batch_size]\n",
    "\n",
    "    data = data.view(batch_size, num_batches)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7528d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data = get_data(tokenized_dataset['test'], vocab, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "16f27f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 9439]), torch.Size([32, 937]), torch.Size([32, 1111]))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, valid_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3273db",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "dcf40b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hid_dim) \n",
    "        # self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers, dropout_rate, batch_first = True)\n",
    "        self.lstm = nn.LSTM(\n",
    "            emb_dim,\n",
    "            hid_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_other)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            getattr(self.lstm, f'weight_ih_l{i}').data.uniform_(-init_range_other, init_range_other)\n",
    "            getattr(self.lstm, f'weight_hh_l{i}').data.uniform_(-init_range_other, init_range_other)\n",
    "            getattr(self.lstm, f'bias_ih_l{i}').data.zero_()\n",
    "            getattr(self.lstm, f'bias_hh_l{i}').data.zero_()\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "        \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach() #not to be used for gradient computation\n",
    "        cell   = cell.detach()\n",
    "        return hidden, cell\n",
    "        \n",
    "    def forward(self, src, hidden):\n",
    "        #src: [batch_size, seq len]\n",
    "        embedding = self.dropout(self.embedding(src)) \n",
    "        #embedding: [batch-size, seq len, emb dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        #ouput: [batch size, seq len, hid dim]\n",
    "        #hidden: [num_layers * direction, seq len, hid_dim]\n",
    "        output = self.dropout(output)\n",
    "        prediction =self.fc(output)\n",
    "        #prediction: [batch_size, seq_len, vocab_size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fed70a",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a68f4293",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 256\n",
    "hid_dim = 256\n",
    "num_layers = 2\n",
    "dropout_rate = 0.3\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f3f1b8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,672,937 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model =LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel () for p in model.parameters() if p.requires_grad)\n",
    "print(f\"The model has {num_params:,} trainable parameters\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "aafef8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "seq_len = 50\n",
    "clip = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5b1958b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #data [batch size, bunch of tokens]\n",
    "    src = data[:, idx:idx+seq_len]\n",
    "    target = data[:, idx+1:idx+seq_len+1]\n",
    "    return src, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "719a4c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, seq_len, device):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, seq len]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    batch_size = data.shape[0]\n",
    "    #reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #hidden does not need to be in the computational graph for efficiency\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        #prediction: [batch size * seq len, vocab size]  \n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / (num_batches // seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2f95b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1be15b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss: 7.173 | Val. Loss: 6.367 | Train PPL: 1303.480 | Val. PPL: 582.198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Train Loss: 6.780 | Val. Loss: 6.226 | Train PPL: 880.385 | Val. PPL: 505.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Train Loss: 6.613 | Val. Loss: 6.129 | Train PPL: 744.558 | Val. PPL: 458.967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Train Loss: 6.471 | Val. Loss: 6.069 | Train PPL: 645.821 | Val. PPL: 432.121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Train Loss: 6.352 | Val. Loss: 6.021 | Train PPL: 573.746 | Val. PPL: 411.904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Train Loss: 6.247 | Val. Loss: 5.990 | Train PPL: 516.520 | Val. PPL: 399.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Train Loss: 6.155 | Val. Loss: 5.964 | Train PPL: 470.889 | Val. PPL: 389.009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Train Loss: 6.064 | Val. Loss: 5.939 | Train PPL: 429.938 | Val. PPL: 379.463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Train Loss: 5.975 | Val. Loss: 5.929 | Train PPL: 393.506 | Val. PPL: 375.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Train Loss: 5.894 | Val. Loss: 5.900 | Train PPL: 362.841 | Val. PPL: 365.148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Train Loss: 5.820 | Val. Loss: 5.894 | Train PPL: 336.884 | Val. PPL: 362.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Train Loss: 5.752 | Val. Loss: 5.869 | Train PPL: 314.858 | Val. PPL: 353.774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Train Loss: 5.684 | Val. Loss: 5.864 | Train PPL: 294.097 | Val. PPL: 352.071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Train Loss: 5.613 | Val. Loss: 5.849 | Train PPL: 273.877 | Val. PPL: 346.852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Train Loss: 5.548 | Val. Loss: 5.838 | Train PPL: 256.741 | Val. PPL: 343.154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Train Loss: 5.490 | Val. Loss: 5.855 | Train PPL: 242.192 | Val. PPL: 348.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Train Loss: 5.426 | Val. Loss: 5.854 | Train PPL: 227.327 | Val. PPL: 348.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Train Loss: 5.361 | Val. Loss: 5.823 | Train PPL: 212.930 | Val. PPL: 338.130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Train Loss: 5.324 | Val. Loss: 5.833 | Train PPL: 205.226 | Val. PPL: 341.230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Train Loss: 5.289 | Val. Loss: 5.827 | Train PPL: 198.129 | Val. PPL: 339.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Train Loss: 5.259 | Val. Loss: 5.818 | Train PPL: 192.285 | Val. PPL: 336.150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Train Loss: 5.241 | Val. Loss: 5.819 | Train PPL: 188.787 | Val. PPL: 336.743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Train Loss: 5.227 | Val. Loss: 5.811 | Train PPL: 186.199 | Val. PPL: 334.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Train Loss: 5.211 | Val. Loss: 5.811 | Train PPL: 183.214 | Val. PPL: 334.044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Train Loss: 5.191 | Val. Loss: 5.819 | Train PPL: 179.561 | Val. PPL: 336.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Train Loss: 5.202 | Val. Loss: 5.804 | Train PPL: 181.668 | Val. PPL: 331.589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Train Loss: 5.182 | Val. Loss: 5.803 | Train PPL: 177.958 | Val. PPL: 331.389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Train Loss: 5.174 | Val. Loss: 5.804 | Train PPL: 176.637 | Val. PPL: 331.542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Train Loss: 5.161 | Val. Loss: 5.805 | Train PPL: 174.268 | Val. PPL: 331.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Train Loss: 5.166 | Val. Loss: 5.803 | Train PPL: 175.220 | Val. PPL: 331.254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | Train Loss: 5.168 | Val. Loss: 5.805 | Train PPL: 175.562 | Val. PPL: 332.008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 | Train Loss: 5.194 | Val. Loss: 5.801 | Train PPL: 180.271 | Val. PPL: 330.565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 | Train Loss: 5.182 | Val. Loss: 5.796 | Train PPL: 178.014 | Val. PPL: 329.064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Train Loss: 5.165 | Val. Loss: 5.795 | Train PPL: 175.059 | Val. PPL: 328.587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 | Train Loss: 5.169 | Val. Loss: 5.791 | Train PPL: 175.813 | Val. PPL: 327.356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | Train Loss: 5.167 | Val. Loss: 5.796 | Train PPL: 175.387 | Val. PPL: 328.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 | Train Loss: 5.176 | Val. Loss: 5.802 | Train PPL: 176.905 | Val. PPL: 330.933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 | Train Loss: 5.185 | Val. Loss: 5.799 | Train PPL: 178.582 | Val. PPL: 329.860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | Train Loss: 5.176 | Val. Loss: 5.795 | Train PPL: 176.921 | Val. PPL: 328.765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | Train Loss: 5.198 | Val. Loss: 5.794 | Train PPL: 180.889 | Val. PPL: 328.353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | Train Loss: 5.208 | Val. Loss: 5.796 | Train PPL: 182.670 | Val. PPL: 329.126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 | Train Loss: 5.225 | Val. Loss: 5.798 | Train PPL: 185.789 | Val. PPL: 329.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 | Train Loss: 5.223 | Val. Loss: 5.800 | Train PPL: 185.412 | Val. PPL: 330.156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 | Train Loss: 5.224 | Val. Loss: 5.800 | Train PPL: 185.676 | Val. PPL: 330.288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 | Train Loss: 5.219 | Val. Loss: 5.800 | Train PPL: 184.733 | Val. PPL: 330.374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Train Loss: 5.220 | Val. Loss: 5.800 | Train PPL: 184.998 | Val. PPL: 330.390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 | Train Loss: 5.219 | Val. Loss: 5.800 | Train PPL: 184.705 | Val. PPL: 330.416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 | Train Loss: 5.219 | Val. Loss: 5.800 | Train PPL: 184.669 | Val. PPL: 330.429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 | Train Loss: 5.217 | Val. Loss: 5.800 | Train PPL: 184.407 | Val. PPL: 330.442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Train Loss: 5.216 | Val. Loss: 5.800 | Train PPL: 184.172 | Val. PPL: 330.454\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=1)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, seq_len, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, seq_len, device)\n",
    "    \n",
    "    lr_scheduler.step(valid_loss)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f} | '\n",
    "          f'Train PPL: {math.exp(train_loss):7.3f} | Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4a7dc0",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "05b99652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 5.831 | Test PPL: 340.857\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-model.pt'))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758df0f2",
   "metadata": {},
   "source": [
    "## 7. Real World Case Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a889c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    \n",
    "    tokens = tokenizer(prompt)\n",
    "    input_ids = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "    input_tensor = torch.LongTensor(input_ids).unsqueeze(0).to(device)  # [1, seq len]\n",
    "    \n",
    "    hidden = model.init_hidden(1, device)\n",
    "    \n",
    "    generated_tokens = tokens.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_seq_len):\n",
    "            prediction, hidden = model(input_tensor, hidden)\n",
    "            prediction = prediction[:, -1, :]  # get the last time step prediction\n",
    "            \n",
    "            # apply temperature\n",
    "            prediction = prediction / temperature\n",
    "            \n",
    "            # get probabilities\n",
    "            probs = torch.softmax(prediction, dim=-1)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1).item()\n",
    "            next_token = itos[next_token_id]\n",
    "            \n",
    "            generated_tokens.append(next_token)\n",
    "            \n",
    "            # prepare input for next time step\n",
    "            input_tensor = torch.LongTensor([[next_token_id]]).to(device)\n",
    "    \n",
    "    return ' '.join(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea0caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.3\n",
      "नेपाल एक गते <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Temperature: 0.5\n",
      "नेपाल एक दिन <unk> <unk> एक <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Temperature: 0.7\n",
      "नेपाल एक दिन <unk> <unk> एक <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ) <unk> <unk> <unk>\n",
      "\n",
      "Temperature: 1.0\n",
      "नेपाल एक दिन अघि बढ्नु एक वर्षमै के लिएर अर्काे रोगको वर्ष लिंदा यो <unk> होलान् । यो अन्य बाहिरी <unk> <unk>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"नेपाल एक \"\n",
    "prompt = \"नेपाल एक सुन्दर देश हो ।\"\n",
    "max_len = 20\n",
    "tokenizer = nepali_tokenizer\n",
    "seed = SEED\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes with a tradeoff of less-make-sense sentence\n",
    "temperature = [0.3, 0.5, 0.7, 1.0]\n",
    "for temp in temperature:\n",
    "    generation = generate(prompt, max_len, temp, model, tokenizer, vocab, device, seed)\n",
    "    print(f\"Temperature: {temp}\\n{generation}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ece22",
   "metadata": {},
   "source": [
    "# 📋 Project Summary: LSTM Language Model for Nepali Text\n",
    "\n",
    "## 🎯 Project Overview\n",
    "This notebook implements a complete **LSTM-based Language Model** for generating Nepali text. The project demonstrates the full pipeline from data preprocessing to model training and text generation.\n",
    "\n",
    "## 🏗️ Architecture & Components\n",
    "\n",
    "### 1. **Data Pipeline**\n",
    "- **Dataset**: Uses `Sakonii/nepalitext-language-model-dataset` from Hugging Face\n",
    "- **Preprocessing**: Custom Nepali tokenizer, vocabulary building with frequency filtering\n",
    "- **Data Split**: 10,000 train, 1,000 validation, 1,000 test samples\n",
    "- **Tokenization**: Space-based tokenization for Nepali text\n",
    "- **Vocabulary**: Built with minimum frequency threshold (≥3 occurrences)\n",
    "\n",
    "### 2. **Model Architecture**\n",
    "- **Type**: Multi-layer LSTM Language Model\n",
    "- **Layers**: 2 LSTM layers with dropout (0.3)\n",
    "- **Dimensions**: \n",
    "  - Embedding: 256 dimensions\n",
    "  - Hidden: 256 dimensions\n",
    "  - Vocabulary: ~2,000+ tokens\n",
    "- **Parameters**: ~2.1M trainable parameters\n",
    "\n",
    "### 3. **Training Configuration**\n",
    "- **Optimizer**: Adam (lr=1e-3)\n",
    "- **Loss**: Cross-Entropy Loss\n",
    "- **Batch Size**: 32\n",
    "- **Sequence Length**: 50 tokens\n",
    "- **Epochs**: 50 (with early stopping via validation loss)\n",
    "- **Learning Rate Scheduling**: ReduceLROnPlateau\n",
    "- **Gradient Clipping**: 0.25\n",
    "\n",
    "## 📊 Key Features\n",
    "\n",
    "### Data Processing\n",
    "- Custom vocabulary with `<unk>` and `<eos>` tokens\n",
    "- Efficient batch processing for training\n",
    "- Memory-optimized data loading\n",
    "\n",
    "### Model Implementation\n",
    "- Proper weight initialization\n",
    "- Hidden state management for sequence generation\n",
    "- Temperature-controlled text generation\n",
    "- GPU/CPU compatibility\n",
    "\n",
    "### Training & Evaluation\n",
    "- Progress tracking with tqdm\n",
    "- Validation-based model saving\n",
    "- Perplexity calculation for model evaluation\n",
    "- Learning rate adaptation\n",
    "\n",
    "## 🔬 Experimental Results\n",
    "\n",
    "### Temperature Effects on Generation\n",
    "The notebook demonstrates how **temperature parameter** affects text generation quality:\n",
    "\n",
    "- **Temperature 0.3**: Conservative, predictable text (may be repetitive)\n",
    "- **Temperature 0.5**: Balanced creativity and coherence  \n",
    "- **Temperature 0.7**: Moderate creativity with some novel combinations\n",
    "- **Temperature 1.0**: High creativity, more diverse but potentially incoherent\n",
    "\n",
    "### Performance Metrics\n",
    "- **Training Loss**: Tracks model convergence\n",
    "- **Validation Loss**: Prevents overfitting\n",
    "- **Perplexity**: Measures model confidence (lower is better)\n",
    "\n",
    "## 🛠️ Technical Implementation\n",
    "\n",
    "### Key Functions\n",
    "- `nepali_tokenizer()`: Custom tokenization for Nepali text\n",
    "- `get_data()`: Efficient batch preparation\n",
    "- `LSTMLanguageModel`: PyTorch model class\n",
    "- `train()` & `evaluate()`: Training and validation loops\n",
    "- `generate()`: Temperature-controlled text generation\n",
    "\n",
    "### Dependencies\n",
    "- **PyTorch**: Deep learning framework\n",
    "- **Hugging Face Datasets**: Data loading\n",
    "- **NumPy**: Numerical operations\n",
    "- **TQDM**: Progress bars\n",
    "- **Collections**: Vocabulary building\n",
    "\n",
    "## 🎨 Text Generation Examples\n",
    "\n",
    "The model can generate coherent Nepali text continuations:\n",
    "- Input: \"नेपाल एक\" (Nepal is)\n",
    "- Output: Context-aware Nepali text with proper grammar patterns\n",
    "\n",
    "## 🔧 Model Limitations & Future Improvements\n",
    "\n",
    "### Current Limitations\n",
    "- Small training dataset (10K samples)\n",
    "- Limited vocabulary coverage\n",
    "- CPU-only training (can be GPU-accelerated)\n",
    "- Basic tokenization (could use subword tokenization)\n",
    "\n",
    "### Potential Enhancements\n",
    "- **Larger Dataset**: Train on more Nepali text\n",
    "- **Advanced Tokenization**: BPE or SentencePiece\n",
    "- **Model Architecture**: Transformer-based models (GPT-style)\n",
    "- **Fine-tuning**: Domain-specific adaptation\n",
    "- **Evaluation Metrics**: BLEU, ROUGE scores\n",
    "\n",
    "## 📚 Learning Outcomes\n",
    "\n",
    "This project demonstrates:\n",
    "- **End-to-end NLP pipeline** implementation\n",
    "- **PyTorch best practices** for language models\n",
    "- **Hyperparameter tuning** effects on generation\n",
    "- **Dataset preprocessing** for low-resource languages\n",
    "- **Model evaluation** and interpretation techniques\n",
    "\n",
    "## 🚀 Deployment Ready\n",
    "\n",
    "The trained model can be:\n",
    "- **Integrated into web applications** (as shown in the React+Django app)\n",
    "- **Used for text completion** tasks\n",
    "- **Fine-tuned** on specific domains\n",
    "- **Extended** with attention mechanisms or transformers\n",
    "\n",
    "---\n",
    "\n",
    "**🎓 Academic Project**: <u>NLP Course Assignment - LSTM Language Model Implementation</u><br/>\n",
    "**👨‍💻 Author**: Rahul Shakya - st125982<br/>\n",
    "**🏫 Institution**: Asian Institute of Technology (AIT) - Semester II"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
